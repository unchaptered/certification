alias k="kubectl"

k get pods
k get pods
k get pods -A --field-selector=status.phase=Running

k run nginx --image=nginx --restart=Never

k get pods -A

k describe pod newpods-9pgc8 | grep Image
k describe pod newpods-g65l6 | grep Image
k describe pod newpods-qnhgw | grep Image

k delete pod -l name=busybox-prod

k scale rs new-replica-set --replicas=5

ê°™ì€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ íŒŒë“œê°„ í†µì‹  ê²½ë¡œ => redis:PORT
ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ íŒŒë“œê°„ í†µì‹  ê²½ë¡œ => redis.marketing.pod.cluster.local:PORT

k create ns dev-ns
k run nginx-pod --image=nginx:alpine
k run custom-nginx --image=nginx --port=8080
k run redis --image=redis:alpine --labels=tier=db
k create service clusterip redis-service --tcp=6379:6379
k create deployment webapp --image=kodekloud/webapp-color --replicas=3
k create deployment redis-deploy -n dev-ns --image=redis --replicas=2

k run httpd --image=httpd:alpine --port=80 --expose
k create service clusterip httpd --tcp=80:80

k get nodes --show-labels
k run nginx --image=nginx --overrides='{"spec":{"nodeName":"node01"}}' 

k run nginx --image=nginx --overrides='{"spec":{"nodeName":"controlplane"}}' 

# Labels and Selector
k get pods --show-labels | grep env=prod
k get pods --selector env=dev
k get all --selector env=prod
k get pods --selector env=prod,bu=finance,tier=frontend

# Taints and tolerations
k taint nodes node01 spray=mortein:NoSchedule
k taint nodes node01 spray=mortein:NoSchedule-

k run mosquito --image=nginx
k label pods mosquito spray=mortein
k run bee --image=nginx \
  --overrides='{
    "spec": {
      "tolerations": [{
        "key": "spray",
        "operator": "Equal",
        "value": "mortein",
        "effect": "NoSchedule"
      }]
    }
  }'

k taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-

# Node Aiffnity
k get nodes node01 -o json | jq '.metadata.labels'
k get nodes node01 -o json | jq '.metadata.labels | length'
k get nodes node01 -o json | jq '.metadata.labels["beta.kubernetes.io/arch"]'

k edit deploy blue
# .spec.template.spec.containers í•˜ìœ„ì— ì½”ë“œ ì¶”ê°€
```shell
spec:
  template:
    spec:
      ...
      affinity:           # ğŸ”§ affinityë¥¼ ì—¬ê¸°ë¡œ ì´ë™
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
```
k rollout restart deploy blue

k edit deploy red
```shell
spec:
  template:
    spec:
      ...
      affinity:           # ğŸ”§ affinityë¥¼ ì—¬ê¸°ë¡œ ì´ë™
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
```
k rollout restart deploy red


k apply -f dasemonset.yaml
```
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
  labels:
     app: elasticsearch
spec:
  selector:
    matchLabels:
      app: elasticsearch

  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: registry.k8s.io/fluentd-elasticsearch:1.20
```

## static pods
# https://kubernetes.io/ko/docs/tasks/configure-pod-container/static-pod/
crictl pods
kubectl get pods -o wide -A | grep "\-controlplane"

```shell
kube-system    etcd-controlplane                      1/1     Running   0          6m39s   192.168.75.155    controlplane   <none>           <none>
kube-system    kube-apiserver-controlplane            1/1     Running   0          6m39s   192.168.75.155    controlplane   <none>           <none>
kube-system    kube-controller-manager-controlplane   1/1     Running   0          6m39s   192.168.75.155    controlplane   <none>           <none>
kube-system    kube-scheduler-controlplane            1/1     Running   0          6m39s   192.168.75.155    controlplane   <none>           <none>
```

cd /etc/kubernetes/manifests

# Create static pod
apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
spec:
  containers:
  - name: static-busybox-container
    image: busybox
    command:
    - sllep 1000

cp sample.ymal /etc/kubernetes/manifests/static-busybox.yaml

# Modify static pod
k run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifest/static-busybox.yaml

# SSH node01
ssh node01
ps -ef  | grep /usr/bin/kubelet
```shell
root       12061       1  0 07:21 ?        00:00:02 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10
root       15879   14369  0 07:27 pts/0    00:00:00 grep /usr/bin/kubelet
```
cat /var/lib/kubelet/config.yaml 
cat /var/lib/kubelet/config.yaml | grep "staticPodPath:"

ls
rm -rf greenbox.yaml

# Multiple Scheduler
k get sa -n kube-system
k get clusterrolebinding
k get clusterrole
kubectl get clusterrolebinding

k get configmap
```yaml
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system
```

k describe pod/kube-scheduler-controlplane -n kube-system | grep "Image:"
```shell  
Image:         registry.k8s.io/kube-scheduler:v1.31.0
```

# Logging and Monitoring - Monitor Cluster Components
curl -LO https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
k apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl top node

# Logging and Monitoring - Managing Application Logs
k logs pod/webbapp-1

# Application Lifecycle Management - Rolling Updates and Rollbacks
k edit deploy/frontend
```yaml
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: Recreate
```

# Application Lifecycle Management - Command and Arguments
k run webapp-green --image=kodekloud/webapp-color --commands -- --color green
k run webapp-green --image=kodekloud/webapp-color -- --color green

```Dockerfile
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]
```
python app.py --color red

# Application Lifecycle Management - Env Variables
k run webapp-color --labels=name=webapp-color --image=kodekloud/webapp-color --env="APP_COLOR=green"
k create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

k apply -f webapp-color.yaml
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
```

# Application Lifecycle Management - Secrets
k create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
```yaml
---
apiVersion: v1 
kind: Pod 
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default 
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    envFrom:
    - secretRef:
        name: db-secret
```

# Application Lifecycle Management - Multi Container Pods
k apply -f yellow.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command:
    - sleep
    - "1000"
  - image: redis
    name: gold
  restartPolicy: Always
```

# Application Lifecycle Management - Init Containers
k apply -f red.yaml
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
```

# Cluster Maintenance - OS Upgrade (node)
k drain node01 --ignore-daemonsets
k uncordon node01
```shell
# Question
error: unable to drain node "node01" due to error: cannot delete cannot delete Pods that declare no controller (use --force to override): default/hr-app, continuing command...
There are pending nodes to be drained:
 node01
cannot delete cannot delete Pods that declare no controller (use --force to override): default/hr-app

# Solution
Run: kubectl get pods -o wide and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.
The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.
```

k drain node01 --ignore-daemonsets --force
k cordon node01

# Cluster Maintenance - Cluster Upgrade Process
k version
kubeadm upgrade plan # íƒ€ì¼“ ë²„ì „ ë³´ëŠ” ë²•

## ì¿ ë²„ë„¤í‹°ìŠ¤ ì»¨íŠ¸ë¡¤í”Œë ˆì¸ ì—…ê·¸ë ˆì´ë“œí•˜ëŠ” ë²• [STEP 1~5]
k drain controlplane --ignore-daemonsets

## STEP 1 - ë³€ê²½í•œ í›„ íŒŒì¼ì„ ì €ì¥í•˜ê³  í…ìŠ¤íŠ¸ í¸ì§‘ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.
cat /etc/apt/sources.list.d/kubernetes.list
vi  /etc/apt/sources.list.d/kubernetes.list
```ini
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
```

## STEP 2 - apt-cache madisonì´ í‘œì‹œí•˜ëŠ” ë²„ì „ ì •ë³´ì— ë”°ë¥´ë©´, Kubernetes ë²„ì „ 1.31.0ì˜ ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ë²„ì „ì´ 1.31.0-1.1ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ Kubernetes v1.31.0ìš© kubeadmì„ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
apt update
apt-cache madison kubeadm
apt-get install kubeadm=1.31.0-1.1

## STEP 3 - ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ë¥¼ ì—…ê·¸ë ˆì´ë“œí•œë‹¤. (ìˆ˜ ë¶„ ê±¸ë¦¼)
kubeadm upgrade plan v1.31.0
kubeadm upgrade apply v1.31.0

## STEP 4 - ì´ì œ Kubelet ë²„ì „ì„ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”. ë˜í•œ ë…¸ë“œ(ì´ ê²½ìš° â€œcontrolplaneâ€ ë…¸ë“œ)ë¥¼ ìŠ¤ì¼€ì¤„ ê°€ëŠ¥ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
apt-get install kubelet=1.31.0-1.1

## STEP 5 - ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ systemd êµ¬ì„±ì„ ìƒˆë¡œ ê³ ì¹˜ê³  Kubelet ì„œë¹„ìŠ¤ì— ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•©ë‹ˆë‹¤.
systemctl daemon-reload
systemctl restart kubelet

## STEP 6 - FIN.
k uncordon controlplane

## ì¿ ë²„ë„¤í‹°ìŠ¤ ë…¸ë“œ ì—…ê·¸ë ˆì´ë“œí•˜ëŠ” ë²•
ssh node01

## STEP 1 - ë³€ê²½í•œ í›„ íŒŒì¼ì„ ì €ì¥í•˜ê³  í…ìŠ¤íŠ¸ í¸ì§‘ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.
cat /etc/apt/sources.list.d/kubernetes.list
vi  /etc/apt/sources.list.d/kubernetes.list
```ini
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
```

## STEP 2 - apt-cache madisonì´ í‘œì‹œí•˜ëŠ” ë²„ì „ ì •ë³´ì— ë”°ë¥´ë©´, Kubernetes ë²„ì „ 1.31.0ì˜ ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ë²„ì „ì´ 1.31.0-1.1ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ Kubernetes v1.31.0ìš© kubeadmì„ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
apt update

apt-cache madison kubeadm

## STEP 3 - apt-cache madisonì´ í‘œì‹œí•˜ëŠ” ë²„ì „ ì •ë³´ì— ë”°ë¥´ë©´, Kubernetes ë²„ì „ 1.31.0ì˜ ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ë²„ì „ì´ 1.31.0-1.1ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ Kubernetes v1.31.0ìš© kubeadmì„ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
apt-get install kubeadm=1.31.0-1.1
kubeadm upgrade node

## STEP 4 - ì´ì œ Kubelet ë²„ì „ì„ ì—…ê·¸ë ˆì´ë“œí•©ë‹ˆë‹¤.
systemctl daemon-reload
systemctl restart kubelet

## STEP 5 - FIN.
k uncordon node01

# Cluster Maintenance - Backup and Restore Methods
etcdctl --version
k describe pod/etcd-controlplane -n kube-system | grep Image

# ETCDì— ë“¤ì–´ê°€ëŠ” ì•¤ë“œí¬ì¸íŠ¸ëŠ” --listen-client-urls ê²½ë¡œì— ìˆëŠ” ê²½ë¡œë¥¼ ì°¸ì¡°
```shell
  Command:
    # ...
    --listen-client-urls=https://127.0.0.1:2379,https://192.168.227.83:2379
```

# ETCD ì„œë²„ì˜ ìœ„ì¹˜
```shell
  Command:
    # ...
    --cert-file=/etc/kubernetes/pki/etcd/server.crt
```

# ETCD ì¸ì¦ì„œ ìœ„ì¹˜
```shell
  Command:
    # ...
    --cert-file=/etc/kubernetes/pki/etcd/ca.crt
```

# ë°±ì—…í•˜ëŠ” ë°©ë²•...
find /opt/snapshot-pre-boot.db
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/snapshot-pre-boot.db

find /opt/snapshot-pre-boot.db

# ë³µêµ¬í•˜ëŠ” ë°©ë²•...
ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup \
  snapshot restore /opt/snapshot-pre-boot.db

cat /etc/kubernetes/manifests/etcd.yaml | grep -A 9 "volumes"
vi  /etc/kubernetes/manifests/etcd.yaml
```yaml
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
```

crictl ps
